{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48Uztv6L9AVC"
   },
   "source": [
    "# Introduction\n",
    "We hope you're reading this on <font color = \"orange\">Google Colab</font>. If not, go back to Part III and follow the guide! \n",
    "\n",
    "In this section, we will:\n",
    "1. Mount the Google Drive and read the CSV\n",
    "2. Train a vanilla LSTM model using Open\n",
    "3. Train another vanilla LSTM model using FilteredOpen\n",
    "4. Plot and compare our model results\n",
    "\n",
    "As mentioned at the end of Part I, we will be answering two questions:\n",
    "1. Can LSTM help with stock predictions?\n",
    "2. Can using signal processing technique help with improving stock predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5AFJIJbth-D"
   },
   "source": [
    "### Step 1: Import pandas\n",
    "Let's start with importing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ytn5JKeRuC8J"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwoDhp0R80m0"
   },
   "source": [
    "### Step 2: Mount your drive\n",
    "Before we can read the CSV we'll need to mount the drive.\n",
    "\n",
    "![MountDriveInstructions](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/MountDriveInstructions.png)\n",
    "\n",
    "Steps you'll need:\n",
    "1. Connect the runtime\n",
    "2. Mount your Google Drive\n",
    "3. Navigate through your directory until you reach the folder containing \"Project Finance x LSTM (Part IV).ipynb\" and the CSV from Part III\n",
    "4. [Not shown] Right click, and click 'Copy path'\n",
    "5. Use that to read your CSV using pandas later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xtYGPuw7U0X"
   },
   "outputs": [],
   "source": [
    "# Step 2: Click on 'Mount Drive' button (2) and mount drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ0DbA6ASWAr"
   },
   "source": [
    "### Step 3: Read the CSV from Part III\n",
    "Now that you've mounted the Drive, you can now read the CSV that you've uploaded into the Google Drive.\n",
    "\n",
    "Make sure you set the first column as your index, and parse dates so that the dates are parsed as DateTimeIndex object.\n",
    "\n",
    "You should have:\n",
    "- 4,904 rows\n",
    "- 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "RayddKta2k5l",
    "outputId": "e5397db7-667b-4822-db50-a8c041370196"
   },
   "outputs": [],
   "source": [
    "# Step 3: Read the CSV from Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7UNR47LeXXK"
   },
   "source": [
    "### Step 4: Import libraries\n",
    "Now, we import the rest of the libraries needed for LSTM model training. Here are the libraries you'll need:\n",
    "- matplotlib.pyplot as plt\n",
    "- numpy as np\n",
    "- StandardScaler from sklearn.preprocessing\n",
    "- mean_squared_error from sklearn.metrics\n",
    "- Sequential from keras.models\n",
    "- Dense from keras.layers\n",
    "- LSTM from keras.layers\n",
    "- Dropout from keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-cCPpMZB4cv"
   },
   "outputs": [],
   "source": [
    "# Step 4: Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpGeeoMYhAGH"
   },
   "source": [
    "### Step 5: Split your data into train and test\n",
    "Similarly, split your DataFrame into train and test DataFrames according to the following dates:\n",
    "\n",
    "- Train data: January 3 2000 to May 16 2019\n",
    "- Test data: May 15 2019 to July 1 2019\n",
    "\n",
    "You might be wondering why there's an overlap between the last 2 days and first 2 days of Train and Test respectively. \n",
    "\n",
    "We'll explain a bit more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUuMdNxW7FzU"
   },
   "outputs": [],
   "source": [
    "# Step 5: Split DataFrame into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6ci5qQTt8IV"
   },
   "source": [
    "# Data Preparation\n",
    "We'll have to do a bit more data preparation before we start training our model.\n",
    "\n",
    "This step was not mentioned in the publication, but it's good practice to scale your values. \n",
    "\n",
    "We'll prepare three separate sets of scaled training data:\n",
    "1. Open\n",
    "2. FilteredOpen\n",
    "3. ZeroMeanFilteredOpen (we'll get to that soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysalzw0AmyWF"
   },
   "source": [
    "### Step 6: Add a new column called ZeroMeanFilteredOpen\n",
    "![ResearchPaperNormalization](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/ResearchPaperNormalization.png)\n",
    "\n",
    "In the paper, the authors performed zero-mean normalization. What is zero-mean normalization?\n",
    "\n",
    "![ZeroMeanNormalization](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/ZeroMeanNormalization.png)\n",
    "\n",
    "Zero-mean normalization happens when you subtract all values in a column with the overall mean. \n",
    "\n",
    "We will do the same with our denoised Open data and name the new column 'ZeroMeanFilteredOpen'.\n",
    "\n",
    "Here's what we'll do:\n",
    "1. Create a new column 'ZeroMeanFilteredOpen' in the train DataFrame, using the mean of 'FilteredOpen' in train\n",
    "2. Create a new column 'ZeroMeanFilteredOpen' in the test DataFrame, using the mean of 'FilteredOpen' in train\n",
    "\n",
    "It's not the usual normalization, but we should still normalize after splitting, and using data from train set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0cbHJXFNdza",
    "outputId": "a978e504-b755-434d-99bc-dcc76e2cca84"
   },
   "outputs": [],
   "source": [
    "# Step 6a: Get the mean of 'FilteredOpen' from train\n",
    "\n",
    "# Step 6b: Create 'ZeroMeanFilteredOpen' in train\n",
    "\n",
    "# Step 6c: Create 'ZeroMeanFilteredOpen' in test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOtIMBSl_T6K"
   },
   "source": [
    "### Step 7: Transform Open, FilteredOpen, and ZeroMeanFilteredOpen with StandardScaler\n",
    "After creating ZeroMeanFilteredOpen, we'll proceed with feature scaling. \n",
    "\n",
    "This step was not mentioned in the paper, but it's good practice to do so for training. \n",
    "\n",
    "First, let's declare three variables containing a StandardScaler, without any additional parameters. \n",
    "\n",
    "We can't use the same scaler for the three Opens because we'll be using the scaler to un-scale predictions as well so the scaler properties must be specific to each Open data.\n",
    "\n",
    "Then, we'll scale our column values and use that for training.\n",
    "\n",
    "Note: If you have an error that goes <strong>\"Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\"</strong>, you used a Series. \n",
    "\n",
    "Either use a DataFrame containing only the Open/FilteredOpen/ZeroMeanFilteredOpen column, or a (-1, 1) reshape to reshape your the np.array of your Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHhvWe5LmtFF"
   },
   "outputs": [],
   "source": [
    "# Step 7a: Declare a StandardScaler for Open\n",
    "\n",
    "# Step 7b: Call .fit_transform on the 'Open' column values from your train dataset\n",
    "\n",
    "\n",
    "# Step 7c: Declare a StandardScaler for FilteredOpen\n",
    "\n",
    "# Step 7d: Call .fit_transform on the 'FilteredOpen' column values from your train dataset\n",
    "\n",
    "\n",
    "# Step 7e: Declare a StandardScaler for ZeroMeanFilteredOpen\n",
    "\n",
    "# Step 7f: Call .fit_transform on the 'ZeroMeanFilteredOpen' column values from your train dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9TgQa2vFyL5"
   },
   "source": [
    "### Step 8: Prepare create arrays for training\n",
    "A bit of context on why we're creating more arrays. For LSTM, we're taking a sequence of data and predicting an output in the end. \n",
    "\n",
    "In our case, we're taking a window of two prices in sequence, and predicting the next one.\n",
    "\n",
    "![LSTMTrainingWindow](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/LSTMTrainingWindow.png)\n",
    "\n",
    "This is why we had the small overlap - so that we have enough data to predict for the \"first\" day of our test data, which the authors said was 17th May 2019.\n",
    "\n",
    "<strong>You will create a list of length-2 NumPy arrays for your train data, and a NumPy array of prices for your test data.</strong>\n",
    "\n",
    "Something like this for the scaled 'Open':\n",
    "\n",
    "![OpenXyTrainList](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/OpenXyTrainList.png)\n",
    "\n",
    "Expect six variables after running this step. Three sets of X train lists and y train lists for \"Open\", \"FilteredOpen\", and \"ZeroMeanFilteredOpen\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0026ObmNDXqi"
   },
   "source": [
    "<details>\n",
    "    <summary>Click for instructions if you're stuck</summary>\n",
    "    <div>\n",
    "        <ol>\n",
    "            <li>Declare list for X train</li>\n",
    "            <li>Declare list for y train</li>\n",
    "            <li>Loop through your scaled \"Open\" values using a for loop and range function</li>\n",
    "              <ul>\n",
    "                  <li>Start at index 2 and end at the last index, so you'll need to configure your range</li>\n",
    "                  <li>Append your X train list with an array containing items from 1 and 2 indices before, i.e. if I am at index 2, I should be appending a NumPy array containing items from index 0 and 1</li>\n",
    "                  <li>Append your y train list with the item from the current index</li>\n",
    "              </ul>\n",
    "            <li>Don't forget that the scaled values from Step 7 are in a NumPy array as well so you need to reshape your sliced array</li>\n",
    "        </ol>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>Tweak the values around like [i-2:i, y] as your slicing (figure what y is and you'll be fine)</p>\n",
    "    </div>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UVPzBW48EuY"
   },
   "outputs": [],
   "source": [
    "# Step 8a: Prepare X train and y train using scaled \"Open\"\n",
    "\n",
    "# Step 8b: Prepare X train and y train using scaled \"FilteredOpen\"\n",
    "\n",
    "# Step 8c: Prepare X train and y train using scaled \"ZeroMeanFilteredOpen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gUOX6EvLXba"
   },
   "source": [
    "### Step 9: Turn the X train and y train lists into NumPy arrays\n",
    "Now that you have a list of NumPy arrays, time to turn them into a NumPy array of NumPy arrays.\n",
    "\n",
    "Sounds confusing, we know. That's why we had this as a separate instruction.\n",
    "\n",
    "After you turn the list into a NumPy array, you can look at its .shape attribute and get a (4871,2) for train, and (4871,) for test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBAKM9vuMGQm"
   },
   "source": [
    "<details>\n",
    "    <summary>Click once if you need a hint</summary>\n",
    "    <div>\n",
    "        <strong>Google \"convert python list into numpy array\"</strong>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCS045mdLvTK"
   },
   "outputs": [],
   "source": [
    "# Step 9a: Turn open X train list into NumPy array\n",
    "\n",
    "# Step 9b: Turn open y train list into NumPy array\n",
    "\n",
    "\n",
    "# Step 9a: Turn filtered X train list into NumPy array\n",
    "\n",
    "# Step 9b: Turn filtered y train list into NumPy array\n",
    "\n",
    "\n",
    "# Step 9a: Turn open X train list into NumPy array\n",
    "\n",
    "# Step 9b: Turn open y train list into NumPy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o56csleQNdtJ",
    "outputId": "00b02dc3-3eb4-43f6-cc5f-aafd7b83e717"
   },
   "outputs": [],
   "source": [
    "# Optional: Get the shape of your converted arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvvk_aHbNzxv"
   },
   "source": [
    "### Step 10: Reshape your X train NumPy arrays\n",
    "We will now reshape our X train arrays from 2D to 3D.\n",
    "\n",
    "Earlier on, you migth have found that our X train arrays have a shape of (4871, 2). Reshape it such that it becomes (4871, 2, 1), a 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dE4Mpy3a-FEY"
   },
   "outputs": [],
   "source": [
    "# Step 10a: Reshape your \"Open\" X train array\n",
    "\n",
    "# Step 10b: Reshape your \"FilteredOpen\" X train array\n",
    "\n",
    "# Step 10c: Reshape your \"ZeroMeanFilteredOpen\" X train array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCIQUwK3PiR9"
   },
   "source": [
    "# Model building and prediction\n",
    "Now that we've prepared our data, now is the time to build and train the model. Since the authors did not detail their architecture, we'll be using a simple LSTM model architecture in this exercise. \n",
    "\n",
    "Don't worry, it does its job well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQr3QuTBP4S4"
   },
   "source": [
    "### Step 11: Set up the model architecture\n",
    "We'll do the following steps to set up a model.\n",
    "\n",
    "1. Declare a variable, and store a Sequential object\n",
    "2. [First layer] Add a LSTM layer\n",
    "  *   50 units\n",
    "  *   return sequences\n",
    "  *   input shape as a tuple with (2, 1)\n",
    "3. [Second layer] Add a Dropout layer, with a rate of 0.3\n",
    "4. [Third layer] Add an LSTM layer\n",
    "  *   50 units\n",
    "5. [Fourth layer] Add a Dropout layer, with a rate of 0.3\n",
    "6. [Fifth layer] Add a Dense layer\n",
    "  *   1 units\n",
    "\n",
    "That's it, you're done. When you call the model's .summary method, you'll see the following:\n",
    "\n",
    "![LSTMModelArchitecture](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/LSTMModelArchitecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P7RDFSF-bUo"
   },
   "outputs": [],
   "source": [
    "# Step 11: Set up your model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2mqODHBR9Z3"
   },
   "source": [
    "### Step 12: Compile and fit your model with \"Open\" data\n",
    "Now that you're done setting up, let's start with the \"Open\" data. Do the following next:\n",
    "1. Call the compile method\n",
    "  *    Use the 'adam' optimizer\n",
    "  *    Use mean_squared_error as the loss function\n",
    "2. Call the fit method\n",
    "  *    Use the \"Open\" X train and y train data\n",
    "  *    Have 15 epochs\n",
    "  *    Use a batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8R-gWEdb-oof",
    "outputId": "531a6d11-76fa-4cf2-eeb8-0a473ade7b65"
   },
   "outputs": [],
   "source": [
    "# Step 12: Compile and fit your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiYP85ApSp-3"
   },
   "source": [
    "### Step 13: Prepare the \"Open\" test data\n",
    "Repeat what you did in Steps 8-10 for the test set.\n",
    " \n",
    "Use the respective scalers with the corresponding test data. For example, scale the \"Open\" from test using the Scaler from Step 7a.\n",
    "\n",
    "Take note that for the reshape step, the dimensions will be different.\n",
    "\n",
    "For \"Open\" test X train, this is what we anticipate to see in the end after repeating the reshape step:\n",
    "\n",
    "![OpenXTestArray](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/OpenXTestArray.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgNKfpszD6d0"
   },
   "outputs": [],
   "source": [
    "# Step 13: Transform, loop, transform, and reshape \"Open\" test X data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTmnHA_DWo_v"
   },
   "source": [
    "### Step 14: Make your predictions\n",
    "Call the predict method of your model, using the X test data you have prepared in Step 13.\n",
    "\n",
    "The predictions must also be transformed using the .inverse_transform method of your scaler from Step 7. \n",
    "\n",
    "Just ignore any warnings that appear. Don't worry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxM4KJ_CF3F8",
    "outputId": "d3dab0b5-6921-4e2a-da04-9cfb947b35c1"
   },
   "outputs": [],
   "source": [
    "# Step 14a: Make the predict method call\n",
    "\n",
    "# Step 14b: Make the inverse_transform call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwzFHdoHYOpd"
   },
   "source": [
    "### Step 15: Create a DataFrame for your \"Open\" prediction\n",
    "Now that we're done with prediction, let's create a DataFrame because we need the date index for plotting and comparison.\n",
    "\n",
    "Our DataFrame is 31 rows long, starts on the 17th May 2019 and ends on 1st July 2019.\n",
    "\n",
    "You can borrow the index of your original test DataFrame, but don't forget that DataFrame is 33 rows long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s80jx21kYNTO"
   },
   "outputs": [],
   "source": [
    "# Step 15: Create a DataFrame for your \"Open\" prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xlmk64w5bEUY"
   },
   "source": [
    "### Step 16: Plot the \"Open\" prediction with the original test \"Open\"\n",
    "Moment of truth. \n",
    "\n",
    "Let's plot the \"Open\" data from the original test DataFrame, from 17th of May 2019 to 1st of July 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUqxVwkbbnMe"
   },
   "source": [
    "<details>\n",
    "    <summary><font color = 'green'>SPOILERS! Click once for a look to compare our plot and yours</font></summary>\n",
    "    <div>\n",
    "        <img src = 'https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/OpenPredictionPlot.png'>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "RtXgQMcVF_GC",
    "outputId": "b8f90327-c897-4e4b-bbb6-6b0b6fcb78dd"
   },
   "outputs": [],
   "source": [
    "# Step 16: Plot \"Open\" prediction with the original test \"Open\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMJXXPaScIUx"
   },
   "source": [
    "### Step 17: Repeat Steps 11-15 for \"FilteredOpen\" and \"ZeroMeanFilteredOpen\" \n",
    "Now that we've successfully done predictions using data from \"Open\", let's work on \"FilteredOpen\" and \"ZeroMeanFilteredOpen\" next.\n",
    "\n",
    "Don't forget that you have to add the FilteredOpen mean to the predictions for ZeroMeanFilteredOpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYW3bJ9Lcofu",
    "outputId": "b0ea1501-9d2a-4020-aaf5-af8fabed1c0c"
   },
   "outputs": [],
   "source": [
    "# Step 17a: Set up your model architecture for FilteredOpen, compile, and fit \"FilteredOpen\" data (Steps 11-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vX1Tb37dAPk"
   },
   "outputs": [],
   "source": [
    "# Step 17b: Transform, loop, transform, and reshape \"FilteredOpen\" test X data (Step 13)\n",
    "\n",
    "# Step 17c: Make the predict method and inverse_transform call (Step 14)\n",
    "\n",
    "# Step 17d: Create a DataFrame for your \"FilteredOpen\" prediction (Step 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOcbZYMkdUkM",
    "outputId": "0e9ecabc-1834-4a2e-af1a-4ac73d42f44c"
   },
   "outputs": [],
   "source": [
    "# Step 17e: Set up your model architecture for ZeroMeanFilteredOpen, compile, and fit \"ZeroMeanFilteredOpen\" data (Steps 11-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "id": "vY-eIBYseFqz",
    "outputId": "f47c16ba-b2ab-44b5-eadb-6256e5b8d7ca"
   },
   "outputs": [],
   "source": [
    "# Step 17f: Transform, loop, transform, and reshape \"ZeroMeanFilteredOpen\" test X data (Step 13)\n",
    "\n",
    "# Step 17g: Make the predict method and inverse_transform call (Step 14)\n",
    "\n",
    "# Step 17h: Create a DataFrame for your \"ZeroMeanFilteredOpen\" prediction (Step 15)\n",
    "\n",
    "# Step 17i: Add the FilteredOpen mean (Step 6a) from train DataFrame into all values (reverse the normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xokh74PzfGt2"
   },
   "source": [
    "### Step 18: Plot all three predictions with the original test \"Open\"\n",
    "Which predictions did the best? Let's find out by plotting all three sets of predictions on the same plot. \n",
    "\n",
    "If your ZeroMeanFilteredOpen plot is way lower than others, make sure you added the mean to undo the zero-mean normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nMtExx7gLxn"
   },
   "source": [
    "<details>\n",
    "    <summary><font color = 'green'>SPOILERS! Click once for a look to compare our plot and yours</font></summary>\n",
    "    <div>\n",
    "        <img src = 'https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectFinancexLSTM/FinalPredictionPlots.png'>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "koqLQYeGfGA4",
    "outputId": "d160e169-9385-47e5-da55-a3b668676efd"
   },
   "outputs": [],
   "source": [
    "# Step 18: Plot all predictions with the original test \"Open\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZZfmhNVg5Xq"
   },
   "source": [
    "### Step 19: Calculate the RMSE of the three predictions\n",
    "Visually, we know which predictions performed best.\n",
    "\n",
    "However, it's good to put a number on it as well. Let's calculate the RMSE of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTaMZTVxLVL4",
    "outputId": "ef8b59b0-d45b-4047-e060-37b0b977162b"
   },
   "outputs": [],
   "source": [
    "# Step 19a: Print the RMSE of test 'Open' and 'Open' predictions\n",
    "\n",
    "# Step 19b: Print the RMSE of test 'Open' and 'FilteredOpen' predictions\n",
    "\n",
    "# Step 19c: Print the RMSE of test 'Open' and 'ZeroMeanFilteredOpen' predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AClfLKKBhVtI"
   },
   "source": [
    "# The end\n",
    "And that's the end! What a journey; you successfully performed classical and deep learning for S&P 500 stock prices.\n",
    "\n",
    "To recap, you've:\n",
    "1. Read research on stock pricing and retrieved the data\n",
    "2. Investigated the ARIMA terms and performed ARIMA modelling\n",
    "3. Used signal processing techniques to denoise stock data\n",
    "4. Trained an LSTM model to predict stock pricing data\n",
    "\n",
    "You have also answered the two questions that we wanted to ask at the start of this project.\n",
    "\n",
    "Go on, give yourself a pat on the back. We hope this project series has give you more confidence in coding and deep learning. \n",
    "\n",
    "Whatever you learn here is but a tip of the iceberg, and launchpad for bigger and better things to come. \n",
    "\n",
    "If you're keen, here are some more things you can try:\n",
    "- More datasets, e.g., HSI and DJI\n",
    "- More modelling, e.g., more complex LSTM infrastructures\n",
    "\n",
    "Come join us in our Telegram community over at https://bit.ly/UpLevelSG and our Facebook page at https://fb.com/UpLevelSG\n",
    "\n",
    "<strong>Most importantly, UpLevel won't be what it is today without learners like yourself so help us grow by spreading the word and get more subscribers <font color = 'red'><3</font></strong>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project Finance x LSTM (Part IV).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
